# ML 목표

# 1. 손실(Loss)
#     - 많은 머신 러닝 문제들은 손실을 최소화하도록 설계
#     - 손실은 모델이 예측한 값과 예측 헀어야 할 값의 차이를 의미
#     - 학습을 통해 학습 데이터에 대한 Train Loss을 최소
#
# 2. 일반화(Generalization)
#     - 머신 러닝에서는 학습하지 않은 데이터에 대해서도 손실을 최소화 하는 것을 목표
#     - 학습 성능이 좋다라는 의미는 일반화 성능까지 고려한 표현
#     - 보통, 충분한 시험 데이터를 통해 일반화 성능을 추
#
# 3. Underfitting
#     - 학습 데이터와 시험 데이터 모두에 대해서 손실을 충분히 작게하지 못한 경우
#     - 모델을 더 복잡하게 하거나 적합한 모델 서치가 필요
#
# 4. Overfitting
#     - 학습 데이터에 대해서 충분히 작은 손실을 달성했으나 Test에서의 Loss가 큰 경우
#     - 일반화 성능이 떨어지는 것이기에 모델을 단순하게 만들어 해결
#
# 5. 오컴의 면도날 -> 모델의 적절한 복잡도 설정
#     -  주어진 작업을 동등하게 잘 해결하는 방법들이 있다면, 가장 간단한 것을 골라야 한다.
#     - 모델이 단순할수록 성능이 좋지 Underfitting의 취약
#     - 정칙화(regularization)등을 통해 복잡도를 조절해가며 좋은 포인트를 찾아야
#
# 6. No Free Lunch 이론
#     - 하나의 알고리즘으로 모든 가능한 경우에 대한 분휴 작업을 한다고 하였을 때, 학습하지 못한 데이터에 대해서는 무작위 예측과 같은
#       성능을 얻을 수 밖에 없다.
#     - 머신러닝은 만능이 아니며 각 Task에 맞는 학습 알고리즘을 잘  찾아서 사용해야 한다.